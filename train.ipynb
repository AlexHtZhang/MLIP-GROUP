{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D, Dropout, Flatten\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy\n",
    "\n",
    "# train \n",
    "def classifier(n):\n",
    "\t# load pima indians dataset\n",
    "\tdataset = numpy.load(\"train.npy\") # 18643 * 32768\n",
    "\tlabel = numpy.load(\"train_one_hot.npy\") # 18643 * 28\n",
    "\t# split into input (X) and output (Y) variables\n",
    "\tX = dataset\n",
    "\tX = numpy.reshape(X,(X.shape[0],64,64,8))\n",
    "\tY = label[:,n]\n",
    "\t# create model\n",
    "\tinput_img = Input(shape=(64, 64, 8))\n",
    "\tx = Convolution2D(4, (1, 1), activation='relu', padding='same')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\tx = Convolution2D(4, (1, 1), activation='relu', padding='same')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\tx = Convolution2D(2, (1, 1), activation='relu', padding='same')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\tx = Convolution2D(2, (1, 1), activation='relu', padding='same')(input_img)\n",
    "\tx = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\tfc= Flatten()(x)\n",
    "\tfc= Dense(64, activation='relu')(fc)\n",
    "\tfc= Dropout(0.15)(fc)\n",
    "\tfc= Dense(32, activation='relu')(fc)\n",
    "\tfc= Dropout(0.15)(fc)\n",
    "\tfc= Dense(16, activation='relu')(fc)\n",
    "\tfc= Dropout(0.15)(fc)\n",
    "\tfc= Dense(8, activation='relu')(fc)\n",
    "\tfc= Dropout(0.15)(fc)\n",
    "\tfc= Dense(4, activation='relu')(fc)\n",
    "\tfc= Dropout(0.15)(fc)\n",
    "\tfc= Dense(2, activation='relu')(fc)\n",
    "\toutput= Dense(1, activation='sigmoid')(fc)\n",
    "\tmodel=Model(input_img, output)\n",
    "\t# Compile model\n",
    "\tsgd = optimizers.SGD(lr=0.01, momentum=0.9,nesterov=True)\n",
    "\tclass_weight = {0: 0.4*float((Y==0).sum()),1: float((Y==1).sum())}\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\t# Fit the model\n",
    "\tmodel.fit(X, Y, epochs=5, batch_size=128,class_weight=class_weight)\n",
    "\treturn model\n",
    "\n",
    "for n in range(28):\n",
    "\tclassifier(n).save('model_'+str(n)+'.h5')\n",
    "\t\n",
    "#classifier(0).save('model_'+str(0)+'.h5')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.autograd as ag\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import prepare\n",
    "import ResNet18\n",
    "\n",
    "path = os.getcwd()# we can change the path to fit our need\n",
    "data_path = os.path.join(path, 'human-protein/train/')# this path is temporary\n",
    "\n",
    "#order of training set\n",
    "df = pd.read_csv(path+'/data/train_idx.txt', sep = '\\t', header = None)\n",
    "df.columns = ['order']\n",
    "order = list(df['order'])\n",
    "\n",
    "# labels of one hot\n",
    "label = np.load(path +'/data/train_one_hot.npy')\n",
    "\n",
    "# id of pictures\n",
    "trainset = pd.read_csv(path + '/data/train.csv')\n",
    "ls = trainset['Id']\n",
    "num = trainset['Target']\n",
    "\n",
    "#hyper-parameter\n",
    "N = label.shape[0] # Training set size\n",
    "B = 28             # Minibacth size\n",
    "NB = int(N/B)-1        # Number of minibatches\n",
    "T = 2               # Number of epochs\n",
    "criterion = nn.CrossEntropyLoss ()\n",
    "\n",
    "# training preparation\n",
    "if torch.cuda.is_available ():\n",
    "    net = ResNet18.ResNet().cuda()\n",
    "    ltrain = ag.Variable(torch.from_numpy(label).cuda(),requires_grad = False)\n",
    "optimizer = torch.optim .SGD(net. parameters(),lr= 0.001 ,momentum = 0.9)\n",
    "\n",
    "# start training resnet\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for k in range(NB):\n",
    "\n",
    "        idxsmp = k*B # indices of samples for k-th minibatch\n",
    "\n",
    "        xt = prepare.load_batch(ls,order,data_path, B, k)\n",
    "        xt = prepare.normalize(xt)\n",
    "        xtrain = np.moveaxis(xt,[1,2],[2,3])\n",
    "\n",
    "        inputs = ag.Variable(torch.from_numpy(xtrain).cuda(),requires_grad = True)\n",
    "\n",
    "        labels0 = ltrain [ idxsmp:idxsmp+B,0 ]\n",
    "        labels1 = ltrain [ idxsmp:idxsmp+B,1 ]\n",
    "        labels2 = ltrain [ idxsmp:idxsmp+B,2 ]\n",
    "        labels3 = ltrain [ idxsmp:idxsmp+B,3 ]\n",
    "        labels4 = ltrain [ idxsmp:idxsmp+B,4 ]\n",
    "        labels5 = ltrain [ idxsmp:idxsmp+B,5 ]\n",
    "        labels6 = ltrain [ idxsmp:idxsmp+B,6 ]\n",
    "        labels7 = ltrain [ idxsmp:idxsmp+B,7 ]\n",
    "        labels8 = ltrain [ idxsmp:idxsmp+B,8 ]\n",
    "        labels9 = ltrain [ idxsmp:idxsmp+B,9 ]\n",
    "        labels10 = ltrain [ idxsmp:idxsmp+B,10 ]\n",
    "        labels11 = ltrain [ idxsmp:idxsmp+B,11 ]\n",
    "        labels12 = ltrain [ idxsmp:idxsmp+B,12 ]\n",
    "        labels13 = ltrain [ idxsmp:idxsmp+B,13 ]\n",
    "        labels14 = ltrain [ idxsmp:idxsmp+B,14 ]\n",
    "        labels15 = ltrain [ idxsmp:idxsmp+B,15 ]\n",
    "        labels16 = ltrain [ idxsmp:idxsmp+B,16 ]\n",
    "        labels17 = ltrain [ idxsmp:idxsmp+B,17 ]\n",
    "        labels18 = ltrain [ idxsmp:idxsmp+B,18 ]\n",
    "        labels19 = ltrain [ idxsmp:idxsmp+B,19 ]\n",
    "        labels20 = ltrain [ idxsmp:idxsmp+B,20 ]\n",
    "        labels21 = ltrain [ idxsmp:idxsmp+B,21 ]\n",
    "        labels22 = ltrain [ idxsmp:idxsmp+B,22 ]\n",
    "        labels23 = ltrain [ idxsmp:idxsmp+B,23 ]\n",
    "        labels24 = ltrain [ idxsmp:idxsmp+B,24 ]\n",
    "        labels25 = ltrain [ idxsmp:idxsmp+B,25 ]\n",
    "        labels26 = ltrain [ idxsmp:idxsmp+B,26 ]\n",
    "        labels27 = ltrain [ idxsmp:idxsmp+B,27 ]\n",
    "\n",
    "        # Initialize the gradients to zero\n",
    "        optimizer.zero_grad ()\n",
    "        # Forward propagation\n",
    "        x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,x21,x22,x23,x24,x25,x26,x27 = net ( inputs )\n",
    "\n",
    "        # Error evaluation\n",
    "        loss0 = criterion( x0 , labels0 )\n",
    "        loss1 = criterion( x1 , labels1 )\n",
    "        loss2 = criterion( x2 , labels2 )\n",
    "        loss3 = criterion( x3 , labels3 )\n",
    "        loss4 = criterion( x4 , labels4 )\n",
    "        loss5 = criterion( x5 , labels5 )\n",
    "        loss6 = criterion( x6 , labels6 )\n",
    "        loss7 = criterion( x7 , labels7 )\n",
    "        loss8 = criterion( x8 , labels8 )\n",
    "        loss9 = criterion( x9 , labels9 )\n",
    "        loss10 = criterion( x10 , labels10 )\n",
    "        loss11 = criterion( x11 , labels11 )\n",
    "        loss12 = criterion( x12 , labels12 )\n",
    "        loss13 = criterion( x13 , labels13 )\n",
    "        loss14 = criterion( x14 , labels14 )\n",
    "        loss15 = criterion( x15 , labels15 )\n",
    "        loss16 = criterion( x16 , labels16 )\n",
    "        loss17 = criterion( x17 , labels17 )\n",
    "        loss18 = criterion( x18 , labels18 )\n",
    "        loss19 = criterion( x19 , labels19 )\n",
    "        loss20 = criterion( x20 , labels20 )\n",
    "        loss21 = criterion( x21 , labels21 )\n",
    "        loss22 = criterion( x22 , labels22 )\n",
    "        loss23 = criterion( x23 , labels23 )\n",
    "        loss24 = criterion( x24 , labels24 )\n",
    "        loss25 = criterion( x25 , labels25 )\n",
    "        loss26 = criterion( x26 , labels26 )\n",
    "        loss27 = criterion( x27 , labels27 )\n",
    "        loss_01 = loss0+loss1+loss2+loss3+loss4+loss5+loss6+loss7+loss8+loss9+loss10\n",
    "        loss_02 = loss11+loss12+loss13+loss14+loss15+loss16+loss17+loss18+loss19+loss20\n",
    "        loss_03 = loss21+loss22+loss23+loss24+loss25+loss26+loss27\n",
    "        loss = loss_01+loss_02+loss_03\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward() #retain_graph=True)\n",
    "        # Parameter update\n",
    "        optimizer.step ()\n",
    "        # Print averaged loss per minibatch every 100 mini - batches\n",
    "        running_loss += loss.cpu().data[0]\n",
    "\n",
    "        if k % 100 == 99:\n",
    "            print ('[%d, %5d] loss : %.3f' %( epoch + 1, k + 1, running_loss/100 ))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    torch.save(net, 'ResNet18_28outputs_epoch%d.pkl'%(epoch+1))\n",
    "    \n",
    "print ('Finished Training ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
